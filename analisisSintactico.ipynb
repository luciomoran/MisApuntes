{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis sintáctico\n",
    "\n",
    "El **análisis sintáctico** es, en el campo de la lingüística, el análisis de las funciones sintácticas o relaciones de concordancia y jerarquía que guardan las palabras agrupándose entre sí en sintagmas u oraciones.\n",
    "\n",
    "Un analizador sintáctico (o **parser**) es un programa informático que analiza una cadena de símbolos de acuerdo a las reglas de una gramática formal. Usualmente hace parte de un compilador, en cuyo caso, transforma una entrada en un **árbol sintáctico de derivación**.\n",
    "\n",
    "El análisis sintáctico convierte el texto de entrada en otras estructuras (comúnmente árboles), que son más útiles para el posterior análisis y capturan la jerarquía implícita de la entrada. Un **analizador léxico** crea **tokens** de una secuencia de caracteres de entrada y son estos tokens los que son procesados por el analizador sintáctico para construir la estructura de datos, por ejemplo un árbol de análisis o árboles de sintaxis abstracta.\n",
    "\n",
    "**Lenguajes de programación**\n",
    "El uso más común de los analizadores sintácticos es como parte de la fase de análisis de los compiladores. De modo que tienen que analizar el código fuente del lenguaje. Los lenguajes de programación tienden a basarse en gramáticas libres de contexto, debido a que se pueden escribir analizadores rápidos y eficientes para estas.\n",
    "\n",
    "Las gramáticas libres de contexto tienen una expresividad limitada y sólo pueden expresar un conjunto limitado de lenguajes. Informalmente la razón de esto es que la memoria de un lenguaje de este tipo es limitada, la gramática no puede recordar la presencia de una construcción en una entrada arbitrariamente larga y esto es necesario en un lenguaje en el que por ejemplo una variable debe ser declarada antes de que pueda ser referenciada. Las gramáticas más complejas no pueden ser analizadas de forma eficiente. Por estas razones es común crear un analizador permisivo para una gramática libre de contexto que acepta un superconjunto del lenguaje (acepta algunas construcciones inválidas), después del análisis inicial las construcciones incorrectas pueden ser filtradas.\n",
    "\n",
    "Normalmente es fácil definir una gramática libre de contexto que acepte todas las construcciones de un lenguaje pero por el contrario es prácticamente imposible construir una gramática libre de contexto que admita solo las construcciones deseadas. En cualquier caso la mayoría de analizadores no son construidos a mano sino usando generadores automáticos.\n",
    "\n",
    "**Clasificación**\n",
    "La tarea esencial de un analizador es determinar si una determinada entrada puede ser derivada desde el símbolo inicial, usando las reglas de una gramática formal, y como hacer esto, existen esencialmente dos formas:\n",
    "\n",
    "* Analizador sintáctico descendente (Top-Down-Parser): ..un analizador puede empezar con el símbolo inicial e intentar transformarlo en la entrada, intuitivamente esto sería ir dividiendo la entrada progresivamente en partes cada vez más pequeñas, de esta forma funcionan los analizadores LL, un ejemplo es el javaCC. Una mejora en estos parsers se puede logar usando GLR (Generalized Left-to-right Rightmost derivation).\n",
    "\n",
    "* Analizador sintáctico ascendente (Bottom-Up-Parser): un analizador puede empezar con la entrada e intentar llegar hasta el símbolo inicial, intuitivamente el analizador intenta encontrar los símbolos más pequeños y progresivamente construir la jerarquía de símbolos hasta el inicial, los analizadores LR funcionan así y un ejemplo es el Yacc. También existen SLR (Simple LR) o los LALR (Look-ahead LR) como también de los GLL7​ (Generalized Left-to-right Leftmost derivation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizar HTML con Python: HTMLParser\n",
    "El módulo **html.parse** define la clase **HTMLParser** la cual sirve de base para el análisis de archivos de texto con formato en HTML y XHTML. Cuando se vincula un archivo HTML a un objeto HTMLParser, este lo procesará de principio a fin, encontrando las etiquetas de apertura, etiquetas de cierre, datos de texto y otros componentes en el archivo fuente y “procesar” cada uno de estos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "\n",
      "<head>\n",
      "\n",
      "\t<title>Primer PÃ¡gina</title>\n",
      "\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "\t<h1> hola mundo </h1>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "archivo = open('holamundo.html',\"r\")\n",
    " \n",
    "for lineas in archivo.readlines() :\n",
    "    print(lineas)\n",
    " \n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='holamundo.html' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "with open('holamundo.html','r') as file:\n",
    "    parser = HTMLParser()\n",
    "    parser.feed(file.read())\n",
    "    \n",
    "print(file)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "#Script para identificar las etiquetas link presentes en una pagina web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://itcolima.edu.mx/www/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    count += 1\n",
    "    \n",
    "print (count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'holamundo.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9807e6162990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyHTMLParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'holamundo.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'holamundo.html'"
     ]
    }
   ],
   "source": [
    "#Script para identificar la etiqueta <!DOCTYPE html> en un archivo HTML\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "\n",
    "# create a subclass and override the handler methods\n",
    "class MyHTMLParser(HTMLParser):\n",
    "   \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "# instantiate the parser and fed it some HTML\n",
    "parser = MyHTMLParser()\n",
    "\n",
    "with open('C:\\Ejercicios_LA2\\Mis Apuntes\\holamundo.html', \"r\") as f:\n",
    "    page = f.read()\n",
    "    \n",
    "    \n",
    "parser.feed(str(page))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Analizador léxico de la Sentencia Print---\n",
      "---Reconoce: print(expresión);\n",
      "Introduce sentencia print: print(hola)\n",
      "Token('PRINT', 'print')\n",
      "Token('OPEN_PAREN', '(')\n"
     ]
    },
    {
     "ename": "LexingError",
     "evalue": "(None, SourcePosition(idx=6, lineno=-1, colno=-1))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLexingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7aaa57d68e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rply\\lexer.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rply\\lexer.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSourcePosition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLexingError\u001b[0m: (None, SourcePosition(idx=6, lineno=-1, colno=-1))"
     ]
    }
   ],
   "source": [
    "from rply import LexerGenerator\n",
    "#from lexer import lexer\n",
    "\n",
    "\n",
    "class Lexer():\n",
    "    def __init__(self):\n",
    "        self.lexer = LexerGenerator()\n",
    "\n",
    "    def _add_tokens(self):\n",
    "        # Print\n",
    "        self.lexer.add('PRINT', r'print')\n",
    "        # Parenthesis\n",
    "        self.lexer.add('OPEN_PAREN', r'\\(')\n",
    "        self.lexer.add('CLOSE_PAREN', r'\\)')\n",
    "        # Semi Colon\n",
    "        self.lexer.add('SEMI_COLON', r'\\;')\n",
    "        # Operators\n",
    "        self.lexer.add('SUM', r'\\+')\n",
    "        self.lexer.add('SUB', r'\\-')\n",
    "        # Number\n",
    "        self.lexer.add('NUMBER', r'\\d+')\n",
    "        # Ignore spaces\n",
    "        self.lexer.ignore('\\s+')\n",
    "\n",
    "    def get_lexer(self):\n",
    "        self._add_tokens()\n",
    "        return self.lexer.build()\n",
    "\n",
    "#Reconocedor léxico de la Sentencia Print\n",
    "    \n",
    "print(\"---Analizador léxico de la Sentencia Print---\")\n",
    "print(\"---Reconoce: print(expresión);\")\n",
    "    \n",
    "    \n",
    "text_input = input(\"Introduce sentencia print: \")\n",
    "\n",
    "\n",
    "lexer = Lexer().get_lexer()\n",
    "tokens = lexer.lex(text_input)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() - self.right.eval()\n",
    "    \n",
    "class Mul(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() * self.right.eval()\n",
    "    \n",
    "class Sum(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() + self.right.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
